#!/usr/bin/env python3
"""
DS18B20 Embedded Application - Code Quality Metrics Monitor
==========================================================

ì½”ë“œ í’ˆì§ˆ ë©”íŠ¸ë¦­ì„ ìˆ˜ì§‘í•˜ê³  ëª¨ë‹ˆí„°ë§í•˜ëŠ” ë„êµ¬ì…ë‹ˆë‹¤.
- ì½”ë“œ ë¼ì¸ ìˆ˜ (Lines of Code)
- í•¨ìˆ˜/í´ë˜ìŠ¤ ë³µì¡ë„
- í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ ì¶”ì •
- ì˜ì¡´ì„± ë¶„ì„
- ì•„í‚¤í…ì²˜ ì¤€ìˆ˜ë„ ê²€ì‚¬
"""

import re
import json
from datetime import datetime
from pathlib import Path
from typing import Dict, Any

class CodeMetricsCollector:
    def _get_architecture_score(self):
        if "architecture_metrics" in self.metrics:
            return self.metrics["architecture_metrics"]["architecture_score"], 0.3
        return None, None

    def _get_test_score(self):
        if "test_metrics" in self.metrics:
            return self.metrics["test_metrics"]["test_success_rate"], 0.25
        return None, None

    def _get_build_score(self):
        if "build_metrics" in self.metrics:
            build_score = 0.0
            if self.metrics["build_metrics"]["compilation_success"]:
                build_score += 50.0
            ram_score = max(0, 100 - self.metrics["build_metrics"]["ram_usage_percent"])
            flash_score = max(0, 100 - self.metrics["build_metrics"]["flash_usage_percent"])
            build_score += (ram_score + flash_score) / 4
            return build_score, 0.2
        return None, None

    def _get_code_score(self):
        if "code_metrics" in self.metrics:
            code_score = 100.0
            avg_complexity = self.metrics["code_metrics"].get("avg_complexity", 0)
            if avg_complexity > 10:
                code_score -= min(50, (avg_complexity - 10) * 5)
            return code_score, 0.15
        return None, None

    def _get_docs_score(self):
        docs_score = 50.0
        if self.docs_dir.exists():
            doc_files = len(list(self.docs_dir.rglob("*.md")))
            docs_score = min(100.0, doc_files * 10)
        return docs_score, 0.1
    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.src_dir = self.project_root / "src"
    # self.test_dir = self.project_root / "test"  # Remove test dir reference
        self.docs_dir = self.project_root / "docs"
        
        # ë©”íŠ¸ë¦­ ê²°ê³¼ ì €ì¥ì†Œ
        self.metrics = {
            "timestamp": datetime.now().isoformat(),
            "code_metrics": {},
            "architecture_metrics": {},
            "build_metrics": {},
            "quality_score": 0.0
        }

    def collect_code_metrics(self) -> Dict[str, Any]:
        """ì½”ë“œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ (ë¼ì¸ ìˆ˜, ë³µì¡ë„ ë“±) - src only"""
        print("ğŸ“Š Collecting code metrics (src only)...")
        metrics = {
            "total_lines": 0,
            "source_files": 0,
            "header_files": 0,
            "functions": 0,
            "classes": 0,
            "complexity_score": 0.0,
            "files_breakdown": {}
        }
        # ì†ŒìŠ¤ íŒŒì¼ ë¶„ì„ (src only)
        for ext in ['*.cpp', '*.h']:
            for file_path in self.src_dir.rglob(ext):
                # acceleration í´ë” ì œì™¸
                if "acceleration" in file_path.parts:
                    continue
                if file_path.is_file():
                    file_metrics = self._analyze_file(file_path)
                    metrics["total_lines"] += file_metrics["lines"]
                    metrics["functions"] += file_metrics["functions"]
                    metrics["classes"] += file_metrics["classes"]
                    metrics["complexity_score"] += file_metrics["complexity"]
                    if ext == '*.cpp':
                        metrics["source_files"] += 1
                    else:
                        metrics["header_files"] += 1
                    metrics["files_breakdown"][str(file_path.relative_to(self.project_root))] = file_metrics
        # í‰ê·  ë³µì¡ë„ ê³„ì‚°
        if metrics["functions"] > 0:
            metrics["avg_complexity"] = metrics["complexity_score"] / metrics["functions"]
        else:
            metrics["avg_complexity"] = 0.0
        return metrics

    def _analyze_file(self, file_path: Path) -> Dict[str, Any]:
        """ê°œë³„ íŒŒì¼ ë¶„ì„"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except UnicodeDecodeError:
            try:
                with open(file_path, 'r', encoding='latin-1') as f:
                    content = f.read()
            except Exception:
                return {"lines": 0, "functions": 0, "classes": 0, "complexity": 0}
        
        lines = len(content.splitlines())
        
        # í•¨ìˆ˜ ê°œìˆ˜ (ê°„ë‹¨í•œ íŒ¨í„´ ë§¤ì¹­)
        function_pattern = r'^\s*(?:virtual\s+)?(?:static\s+)?(?:inline\s+)?(?:\w+\s+)*\w+\s*\([^)]*\)\s*(?:const\s*)?(?:override\s*)?[{;]'
        functions = len(re.findall(function_pattern, content, re.MULTILINE))
        
        # í´ë˜ìŠ¤ ê°œìˆ˜
        class_pattern = r'^\s*class\s+\w+'
        classes = len(re.findall(class_pattern, content, re.MULTILINE))
        
        # ê°„ë‹¨í•œ ë³µì¡ë„ (if/for/while/switch ë¬¸ ê°œìˆ˜)
        complexity_patterns = [r'\bif\s*\(', r'\bfor\s*\(', r'\bwhile\s*\(', r'\bswitch\s*\(']
        complexity = sum(len(re.findall(pattern, content)) for pattern in complexity_patterns)
        
        return {
            "lines": lines,
            "functions": functions,
            "classes": classes,
            "complexity": complexity
        }

    def collect_architecture_metrics(self) -> Dict[str, Any]:
        """ì•„í‚¤í…ì²˜ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ (Clean Architecture ì¤€ìˆ˜ë„)"""
        print("ğŸ—ï¸ Collecting architecture metrics...")
        
        metrics = {
            "layer_separation": self._check_layer_separation(),
            "dependency_inversion": self._check_dependency_inversion(),
            "interface_usage": self._check_interface_usage(),
            "solid_principles": self._check_solid_principles(),
            "architecture_score": 0.0
        }
        
        # ì•„í‚¤í…ì²˜ ì ìˆ˜ ê³„ì‚° (0-100)
        scores = [
            metrics["layer_separation"]["score"],
            metrics["dependency_inversion"]["score"],
            metrics["interface_usage"]["score"],
            metrics["solid_principles"]["score"]
        ]
        metrics["architecture_score"] = sum(scores) / len(scores)
        
        return metrics

    def _check_layer_separation(self) -> Dict[str, Any]:
        """ê³„ì¸µ ë¶„ë¦¬ í™•ì¸"""
        domain_files = list((self.src_dir / "domain").glob("*.h")) if (self.src_dir / "domain").exists() else []
        application_files = list((self.src_dir / "application").glob("*.h")) if (self.src_dir / "application").exists() else []
        infrastructure_files = list((self.src_dir / "infrastructure").glob("*.h")) if (self.src_dir / "infrastructure").exists() else []
        
        total_files = len(domain_files) + len(application_files) + len(infrastructure_files)
        
        return {
            "domain_files": len(domain_files),
            "application_files": len(application_files),
            "infrastructure_files": len(infrastructure_files),
            "total_layered_files": total_files,
            "score": min(100.0, total_files * 10)  # íŒŒì¼ ìˆ˜ì— ë”°ë¥¸ ì ìˆ˜
        }

    def _check_dependency_inversion(self) -> Dict[str, Any]:
        """ì˜ì¡´ì„± ì—­ì „ ì›ì¹™ í™•ì¸"""
        interfaces = []
        implementations = []
        
        # ì¸í„°í˜ì´ìŠ¤ íŒŒì¼ ì°¾ê¸° (Ië¡œ ì‹œì‘í•˜ëŠ” í´ë˜ìŠ¤)
        for file_path in self.src_dir.rglob("*.h"):
            if file_path.is_file():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    # ì¸í„°í˜ì´ìŠ¤ íŒ¨í„´ ì°¾ê¸°
                    if re.search(r'class\s+I[A-Z]\w+', content):
                        interfaces.append(str(file_path.name))
                    # êµ¬í˜„ í´ë˜ìŠ¤ íŒ¨í„´ ì°¾ê¸° (ì¸í„°í˜ì´ìŠ¤ë¥¼ ìƒì†ë°›ëŠ”)
                    if re.search(r'class\s+\w+\s*:\s*public\s+I[A-Z]\w+', content):
                        implementations.append(str(file_path.name))
                except Exception as e:
                    print(f"[WARN] Failed to read {file_path}: {e}")
                    continue
        
        dip_score = 0.0
        if len(interfaces) > 0:
            dip_score = min(100.0, (len(implementations) / len(interfaces)) * 100)
        
        return {
            "interfaces_count": len(interfaces),
            "implementations_count": len(implementations),
            "interfaces": interfaces,
            "score": dip_score
        }

    def _check_interface_usage(self) -> Dict[str, Any]:
        """ì¸í„°í˜ì´ìŠ¤ ì‚¬ìš©ë¥  í™•ì¸ (I* ì¸í„°í˜ì´ìŠ¤ ê¸°ë°˜)"""
        interface_files = []
        
        # src ë””ë ‰í† ë¦¬ì—ì„œ Ië¡œ ì‹œì‘í•˜ëŠ” ì¸í„°í˜ì´ìŠ¤ íŒŒì¼ ì°¾ê¸°
        for file_path in self.src_dir.rglob("*.h"):
            if file_path.is_file():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    # Ië¡œ ì‹œì‘í•˜ëŠ” í´ë˜ìŠ¤ë‚˜ Mock/Test ê´€ë ¨ í´ë˜ìŠ¤ ì°¾ê¸°
                    if (re.search(r'class\s+I[A-Z]\w+', content) or 
                        re.search(r'class\s+(Mock|Test)[A-Z]\w+', content)):
                        interface_files.append(str(file_path.name))
                except Exception as e:
                    print(f"[WARN] Failed to read {file_path}: {e}")
                    continue
        
        # ì¸í„°í˜ì´ìŠ¤ íŒŒì¼ ìˆ˜ì— ë”°ë¥¸ ì ìˆ˜ ê³„ì‚°
        score = min(100.0, len(interface_files) * 20)  # ì¸í„°í˜ì´ìŠ¤ íŒŒì¼ë‹¹ 20ì 
        
        return {
            "interface_files_count": len(interface_files),
            "interface_files": interface_files,
            "score": score
        }

    def _check_solid_principles(self) -> Dict[str, Any]:
        """SOLID ì›ì¹™ ì¤€ìˆ˜ë„ í™•ì¸"""
        # ë‹¨ìˆœí™”ëœ SOLID ì²´í¬ (íŒŒì¼ í¬ê¸° ë° ì±…ì„ ë¶„ë¦¬)
        large_files = []
        total_files = 0
        
        for file_path in self.src_dir.rglob("*.cpp"):
            if file_path.is_file():
                total_files += 1
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        lines = len(f.readlines())
                    if lines > 200:  # 200ë¼ì¸ ì´ìƒì€ í° íŒŒì¼ë¡œ ê°„ì£¼
                        large_files.append({"file": str(file_path.name), "lines": lines})
                except Exception as e:
                    print(f"Error processing file {file_path}: {e}")
                    continue
        
        # í° íŒŒì¼ì´ ì ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
        score = 100.0
        if total_files > 0:
            large_file_ratio = len(large_files) / total_files
            score = max(0.0, 100.0 - (large_file_ratio * 100))
        
        return {
            "total_source_files": total_files,
            "large_files_count": len(large_files),
            "large_files": large_files,
            "score": score
        }

    # def collect_test_metrics(self) -> Dict[str, Any]:
    #     """í…ŒìŠ¤íŠ¸ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ (removed)"""
    #     return {}

    def _decode_test_log(self, test_log_path):
        for encoding in ['utf-16', 'utf-8', 'latin-1', 'cp1252']:
            try:
                with open(test_log_path, 'r', encoding=encoding) as f:
                    content = f.read()
                print(f"âœ… Successfully decoded with {encoding}")
                return content
            except UnicodeDecodeError:
                continue
        print("âŒ Could not decode test log file")
        return None

    def _parse_unity_test_results(self, content, metrics):
        import re
        test_match = re.search(r'(\d+) Tests (\d+) Failures (\d+) Ignored', content)
        print(f"ğŸ” Regex match result: {test_match}")
        if test_match:
            total_tests = int(test_match.group(1))
            failures = int(test_match.group(2))
            passed = total_tests - failures
            metrics["test_cases"] = total_tests
            metrics["passed_tests"] = passed
            metrics["failed_tests"] = failures
            if total_tests > 0:
                metrics["test_success_rate"] = (passed / total_tests) * 100
            print(f"âœ… Found test results: {total_tests} tests, {passed} passed, {failures} failed")
            return True
        return False

    def _parse_alternative_test_results(self, content, metrics):
        import re
        alt_patterns = [
            r'(\d+) test cases: (\d+) succeeded',
            r'Tests run: (\d+), Failures: (\d+)',
            r'SUMMARY.*(\d+) succeeded.*(\d+) failed'
        ]
        for pattern in alt_patterns:
            alt_match = re.search(pattern, content, re.IGNORECASE | re.DOTALL)
            if alt_match:
                print(f"ğŸ“ Alternative pattern matched: {pattern}")
                # ê°„ë‹¨ ì˜ˆì‹œ: ì²« ë²ˆì§¸ ê·¸ë£¹ì„ test_cases, ë‘ ë²ˆì§¸ë¥¼ passedë¡œ ê°€ì •
                metrics["test_cases"] = int(alt_match.group(1))
                metrics["passed_tests"] = int(alt_match.group(2))
                metrics["failed_tests"] = metrics["test_cases"] - metrics["passed_tests"]
                if metrics["test_cases"] > 0:
                    metrics["test_success_rate"] = (metrics["passed_tests"] / metrics["test_cases"]) * 100
                return True
        return False

    def _parse_test_execution_time(self, content, metrics):
        import re
        time_match = re.search(r'Took (\d+\.\d+) seconds', content)
        if time_match:
            metrics["execution_time"] = float(time_match.group(1))
            print(f"â±ï¸ Execution time: {metrics['execution_time']}s")

    def collect_build_metrics(self) -> Dict[str, Any]:
        """ë¹Œë“œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        print("ğŸ”¨ Collecting build metrics...")
        
        metrics = {
            "compilation_success": False,
            "ram_usage_percent": 0.0,
            "flash_usage_percent": 0.0,
            "binary_size_bytes": 0,
            "warnings_count": 0,
            "errors_count": 0
        }
        
        # ë¹Œë“œ ë¡œê·¸ íŒŒì¼ í™•ì¸
        build_log_path = self.project_root / "logs" / "comfile" / "compile_results.txt"
        
        if build_log_path.exists():
            try:
                content = self._decode_build_log(build_log_path)
                if content is None:
                    return metrics
                print(f"ğŸ“‹ Reading build log: {build_log_path}")
                self._parse_build_log_metrics(content, metrics)
            except Exception as e:
                print(f"âŒ Error reading build log: {e}")
        else:
            print(f"âŒ Build log not found: {build_log_path}")
        
        return metrics

    def _decode_build_log(self, build_log_path):
        for encoding in ['utf-16', 'utf-8', 'latin-1', 'cp1252']:
            try:
                with open(build_log_path, 'r', encoding=encoding) as f:
                    content = f.read()
                print(f"âœ… Successfully decoded build log with {encoding}")
                return content
            except UnicodeDecodeError:
                continue
        print("âŒ Could not decode build log file")
        return None

    def _parse_build_log_metrics(self, content, metrics):
        import re
        ram_match = re.search(r'RAM:\s+\[.*?\]\s+(\d+\.\d+)%', content)
        if ram_match:
            metrics["ram_usage_percent"] = float(ram_match.group(1))
            print(f"ğŸ’¾ RAM Usage: {metrics['ram_usage_percent']}%")
        flash_match = re.search(r'Flash:\s+\[.*?\]\s+(\d+\.\d+)%', content)
        if flash_match:
            metrics["flash_usage_percent"] = float(flash_match.group(1))
            print(f"ğŸ’½ Flash Usage: {metrics['flash_usage_percent']}%")
        if "SUCCESS" in content:
            metrics["compilation_success"] = True
            print("âœ… Compilation: SUCCESS")
        else:
            print("âŒ Compilation: FAILED")
        metrics["warnings_count"] = content.count("warning:")
        metrics["errors_count"] = content.count("error:")
        if metrics["warnings_count"] > 0:
            print(f"âš ï¸ Warnings: {metrics['warnings_count']}")
        if metrics["errors_count"] > 0:
            print(f"ğŸš¨ Errors: {metrics['errors_count']}")
        return metrics

    def calculate_quality_score(self) -> float:
        """ì¢…í•© í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0-100)"""
        scores = []
        weights = []
        for getter in [self._get_architecture_score, self._get_test_score, self._get_build_score, self._get_code_score, self._get_docs_score]:
            score, weight = getter()
            if score is not None and weight is not None:
                scores.append(score)
                weights.append(weight)
        if scores and weights:
            weighted_sum = sum(score * weight for score, weight in zip(scores, weights))
            total_weight = sum(weights)
            return weighted_sum / total_weight if total_weight > 0 else 0.0
        return 0.0

    def _report_code_metrics(self):
        m = self.metrics['code_metrics']
        return f"""
### ğŸ“ˆ Code Metrics
    - **Source Files**: {m['source_files']} (.cpp)
    - **Header Files**: {m['header_files']} (.h)
    - **Total Lines**: {m['total_lines']:,}
    - **Functions**: {m['functions']}
    - **Classes**: {m['classes']}
    - **Average Complexity**: {m['avg_complexity']:.1f}
"""


    def _report_test_metrics(self):
        m = self.metrics['test_metrics']
        return f"""
### ğŸ§ª Test Metrics
- **Test Files**: {m['test_files']}
- **Test Cases**: {m['test_cases']}
- **Success Rate**: {m['test_success_rate']:.1f}%
- **Execution Time**: {m['execution_time']:.3f}s
- **Coverage Estimate**: {m['coverage_estimate']:.1f}%
"""

    def _report_build_metrics(self):
        m = self.metrics['build_metrics']
        return f"""
### ğŸ”¨ Build Metrics
- **Compilation**: {'âœ… Success' if m['compilation_success'] else 'âŒ Failed'}
- **RAM Usage**: {m['ram_usage_percent']:.1f}%
- **Flash Usage**: {m['flash_usage_percent']:.1f}%
- **Warnings**: {m['warnings_count']}
- **Errors**: {m['errors_count']}
"""

    def generate_report(self) -> str:
        self.metrics["code_metrics"] = self.collect_code_metrics()
        self.metrics["architecture_metrics"] = self.collect_architecture_metrics()
        self.metrics["build_metrics"] = self.collect_build_metrics()
        self.metrics["quality_score"] = self.calculate_quality_score()
        report = f"""
# DS18B20 Embedded Application - Code Quality Report
Generated: {self.metrics['timestamp']}
\n## ğŸ“Š Overall Quality Score: {self.metrics['quality_score']:.1f}/100\n"""
        report += self._report_code_metrics()
    # report += self._report_architecture_metrics()  # skipped due to missing method
        report += self._report_build_metrics()
        report += "\n## ğŸ“‹ Recommendations\n\n"
        if self.metrics['quality_score'] < 70:
            report += "- ğŸš¨ Overall quality score is below 70. Consider improving architecture.\n"
        if self.metrics['architecture_metrics']['dependency_inversion']['score'] < 80:
            report += "- ğŸ—ï¸ Consider adding more interfaces to improve dependency inversion.\n"
        if self.metrics['build_metrics']['ram_usage_percent'] > 80:
            report += "- ğŸ’¾ RAM usage is high (>80%). Consider memory optimization.\n"
        if self.metrics['code_metrics']['avg_complexity'] > 10:
            report += "- ğŸ”„ Average function complexity is high. Consider refactoring complex functions.\n"
        return report

    def save_metrics(self, output_path: str):
        """ë©”íŠ¸ë¦­ì„ JSON íŒŒì¼ë¡œ ì €ì¥"""
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(self.metrics, f, indent=2, ensure_ascii=False)
        print(f"ğŸ“ Metrics saved to: {output_path}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ì°¾ê¸°
    import subprocess
    current_dir = Path(__file__).parent
    project_root = current_dir.parent.parent

    print(f"ğŸ” Analyzing project: {project_root}")

    # 1. ë¹Œë“œ ëª…ë ¹ì–´ ë¨¼ì € ì‹¤í–‰
    build_cmd = [
        "pio", "run", "-e", "uno_r4_wifi", "-v"
    ]
    build_log_path = project_root / "logs" / "comfile" / "compile_results.txt"
    print(f"ğŸ”¨ Running build: {' '.join(build_cmd)}")
    with open(build_log_path, "w", encoding="utf-8") as log_file:
        result = subprocess.run(build_cmd, stdout=log_file, stderr=subprocess.STDOUT, cwd=str(project_root))
    print(f"ğŸ”¨ Build finished with exit code {result.returncode}")

    # 2. í’ˆì§ˆ ë¶„ì„ ë¡œì§ ìˆ˜í–‰
    collector = CodeMetricsCollector(str(project_root))

    # ë¦¬í¬íŠ¸ ìƒì„±
    report = collector.generate_report()

    # ê²°ê³¼ ì¶œë ¥
    print(report)

    # ê²°ê³¼ ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # JSON ë©”íŠ¸ë¦­ ì €ì¥
    json_path = project_root / "logs" / "quality" / f"metrics_{timestamp}.json"
    json_path.parent.mkdir(parents=True, exist_ok=True)
    collector.save_metrics(str(json_path))

    # ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ì €ì¥
    md_path = project_root / "logs" / "quality" / f"quality_report_{timestamp}.md"
    with open(md_path, 'w', encoding='utf-8') as f:
        f.write(report)
    print(f"ğŸ“„ Report saved to: {md_path}")

    print(f"\nâœ… Quality analysis complete! Overall score: {collector.metrics['quality_score']:.1f}/100")

if __name__ == "__main__":
    main()
